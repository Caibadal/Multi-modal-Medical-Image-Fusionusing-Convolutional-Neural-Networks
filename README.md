ABSTRACT:

Multimodal image fusion merge images of different modalities into a single image which contains greater information than any of the input images. In medical field, multimodal image fusion plays a crucial role in providing medical practitioners sufficient information about the input images for clinical purposes. In recent years, deep learning (DL) based image fusion has achieved remarkable breakthroughs and state of the art results owing to strong
capability in feature extraction. One of the challenge that is common in the DL based image fusion is the unavailability of ground truth. Therefore, this thesis aims to develop a convolutional neural network based medical image fusion approach on two different 2D modalities without any groundtruth information. The end to end learning framework proposed in this work presents a completely new approach to deal with the problem of multimodal medical image fusion where we obtain a fused image as an output of our network. Our results shows that we achieve promising results after comparing the method with other medical image fusion approaches available in literature.



All the contents of this folder are going to be explained in this document.

-Thesis: LaTeX zip and pdf version.
-Project: all codes used for the results (readme file inside).

